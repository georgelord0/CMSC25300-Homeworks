{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basis for hilbert matrix is: [[ 8.13304645e-01 -5.43794290e-01  1.99095308e-01 -5.51132525e-02\n",
      "   1.19578497e-02 -1.96841135e-03  2.15864623e-04]\n",
      " [ 4.06652323e-01  3.03317262e-01 -6.88560548e-01  4.75965265e-01\n",
      "  -1.97392658e-01  5.41101419e-02 -9.06621362e-03]\n",
      " [ 2.71101548e-01  3.93949644e-01 -2.07134626e-01 -4.90111167e-01\n",
      "   6.10839678e-01 -3.26877926e-01  9.06621671e-02]\n",
      " [ 2.03326161e-01  3.81744394e-01  1.12389156e-01 -4.39598473e-01\n",
      "  -2.54179059e-01  6.41038616e-01 -3.62648656e-01]\n",
      " [ 1.62660929e-01  3.51412668e-01  2.91518455e-01 -1.12276608e-01\n",
      "  -4.99206688e-01 -2.02237770e-01  6.79966236e-01]\n",
      " [ 1.35550774e-01  3.20235052e-01  3.89191824e-01  2.30886766e-01\n",
      "  -1.50594723e-01 -5.41821699e-01 -5.98370285e-01]\n",
      " [ 1.16186378e-01  2.92095792e-01  4.40744903e-01  5.20623748e-01\n",
      "   5.01273333e-01  3.80549160e-01  1.99456764e-01]]\n",
      "If the following matrix is nearly the identity matrix, our basis is orthogonal: \n",
      "[[ 1.00000000e+00 -2.34598687e-11  2.30181916e-10 -8.79269460e-10\n",
      "   1.59333744e-09 -1.36074881e-09  4.42911890e-10]\n",
      " [-2.34598687e-11  1.00000000e+00  2.53766325e-10 -1.09366857e-09\n",
      "   1.92635401e-09 -1.68307149e-09  5.28256230e-10]\n",
      " [ 2.30181916e-10  2.53766325e-10  1.00000000e+00 -7.24370808e-10\n",
      "   1.87573099e-09 -1.29590363e-09  6.18300051e-10]\n",
      " [-8.79269460e-10 -1.09366857e-09 -7.24370808e-10  9.99999998e-01\n",
      "   7.61530038e-10 -1.87295806e-09 -1.44069558e-10]\n",
      " [ 1.59333744e-09  1.92635401e-09  1.87573099e-09  7.61530038e-10\n",
      "   1.00000000e+00  3.23560657e-11  1.37259315e-09]\n",
      " [-1.36074881e-09 -1.68307149e-09 -1.29590363e-09 -1.87295806e-09\n",
      "   3.23560657e-11  9.99999998e-01 -5.55366776e-10]\n",
      " [ 4.42911890e-10  5.28256230e-10  6.18300051e-10 -1.44069558e-10\n",
      "   1.37259315e-09 -5.55366776e-10  1.00000000e+00]]\n",
      "Thus we see the matrix is orthogonal, considering rounding errors\n",
      "The L_1 norm is: 8.075457826289347e-09\n"
     ]
    }
   ],
   "source": [
    "def gram_schmidt(X):\n",
    "    # X is an n-by-p matrix.\n",
    "    # Returns U an orthonormal matrix.\n",
    "    # eps is a threshold value to identify if a vector \n",
    "    # is nearly a zero vector.\n",
    "    eps = 1e-12\n",
    "    \n",
    "    n, p = X.shape\n",
    "    U = np.zeros((n, p))\n",
    "    for j in range(p):\n",
    "        # Get the j-th column of matrix X\n",
    "        v = X[:, j]            \n",
    "        # Write your own code here: Perform the \n",
    "        # orthogonalization by subtracting the projections on \n",
    "        # all columns of U. \n",
    "        for i in range(j):\n",
    "            U_i = U[:,i]\n",
    "            proj = (U_i.T@v)*U_i\n",
    "            v = v-proj\n",
    "\n",
    "        # And then check whether the vector \n",
    "        # you get is nearly a zero vector.\n",
    "        norm = la.norm(v)\n",
    "        if (norm <= eps):\n",
    "            v = v*0\n",
    "        else:\n",
    "            v = v / norm\n",
    "\n",
    "        U[:, j] = v.T\n",
    "    \n",
    "    return U\n",
    "\n",
    "def hilbert_matrix(n):\n",
    "    X = np.array([[1.0 / (i + j - 1) for i in range(1, n + 1)] for j in range(1, n + 1)])\n",
    "    \n",
    "    return X\n",
    "\n",
    "h = hilbert_matrix(7)\n",
    "\n",
    "h_basis = gram_schmidt(h)\n",
    "\n",
    "print(f\"Basis for hilbert matrix is: {h_basis}\")\n",
    "\n",
    "print(f\"If the following matrix is nearly the identity matrix, our basis is orthogonal: \")\n",
    "error = h_basis@h_basis.T\n",
    "print(error)\n",
    "print(f\"Thus we see the matrix is orthogonal, considering rounding errors\")\n",
    "\n",
    "L_1_norm = la.norm(error - np.identity(7))\n",
    "print(f\"The L_1 norm of the error matrix is: {L_1_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basis for hilbert matrix WITH MODIFIED GS is: [[ 8.13304645e-01 -5.43794290e-01  1.99095308e-01 -5.51132525e-02\n",
      "   1.19578497e-02 -1.96841135e-03  2.15864623e-04]\n",
      " [ 4.06652323e-01  3.03317262e-01 -6.88560548e-01  4.75965265e-01\n",
      "  -1.97392658e-01  5.41101419e-02 -9.06621362e-03]\n",
      " [ 2.71101548e-01  3.93949644e-01 -2.07134626e-01 -4.90111167e-01\n",
      "   6.10839678e-01 -3.26877926e-01  9.06621671e-02]\n",
      " [ 2.03326161e-01  3.81744394e-01  1.12389156e-01 -4.39598473e-01\n",
      "  -2.54179059e-01  6.41038616e-01 -3.62648656e-01]\n",
      " [ 1.62660929e-01  3.51412668e-01  2.91518455e-01 -1.12276608e-01\n",
      "  -4.99206688e-01 -2.02237770e-01  6.79966236e-01]\n",
      " [ 1.35550774e-01  3.20235052e-01  3.89191824e-01  2.30886766e-01\n",
      "  -1.50594723e-01 -5.41821699e-01 -5.98370285e-01]\n",
      " [ 1.16186378e-01  2.92095792e-01  4.40744903e-01  5.20623748e-01\n",
      "   5.01273333e-01  3.80549160e-01  1.99456764e-01]]\n",
      "If the following matrix is nearly the identity matrix, our basis is orthogonal: \n",
      "[[ 1.00000000e+00 -2.34598687e-11  2.30181916e-10 -8.79269460e-10\n",
      "   1.59333744e-09 -1.36074881e-09  4.42911890e-10]\n",
      " [-2.34598687e-11  1.00000000e+00  2.53766325e-10 -1.09366857e-09\n",
      "   1.92635401e-09 -1.68307149e-09  5.28256230e-10]\n",
      " [ 2.30181916e-10  2.53766325e-10  1.00000000e+00 -7.24370808e-10\n",
      "   1.87573099e-09 -1.29590363e-09  6.18300051e-10]\n",
      " [-8.79269460e-10 -1.09366857e-09 -7.24370808e-10  9.99999998e-01\n",
      "   7.61530038e-10 -1.87295806e-09 -1.44069558e-10]\n",
      " [ 1.59333744e-09  1.92635401e-09  1.87573099e-09  7.61530038e-10\n",
      "   1.00000000e+00  3.23560657e-11  1.37259315e-09]\n",
      " [-1.36074881e-09 -1.68307149e-09 -1.29590363e-09 -1.87295806e-09\n",
      "   3.23560657e-11  9.99999998e-01 -5.55366776e-10]\n",
      " [ 4.42911890e-10  5.28256230e-10  6.18300051e-10 -1.44069558e-10\n",
      "   1.37259315e-09 -5.55366776e-10  1.00000000e+00]]\n",
      "Thus we see the matrix is orthogonal, considering rounding errors\n",
      "The L_1 norm of the error matrix of basis WITH MODIFIED GS is: 8.075457826289347e-09\n"
     ]
    }
   ],
   "source": [
    "def modified_gram_schmidt(X):\n",
    "    # Define a threshold value to identify if a vector \n",
    "    # is nearly a zero vector.\n",
    "    eps = 1e-12\n",
    "    \n",
    "    n, p = X.shape\n",
    "    U = np.zeros((n, 0))\n",
    "    \n",
    "    for j in range(p):\n",
    "        # Get the j-th column of matrix X\n",
    "        v = X[:, j]\n",
    "        for i in range(j):\n",
    "            # Compute and subtract the projection of \n",
    "            # vector v onto the i-th column of U\n",
    "            v = v - np.dot(U[:, i], v) * U[:, i]\n",
    "        v = np.reshape(v, (-1, 1))\n",
    "        # Check whether the vector we get is nearly \n",
    "        # a zero vector\n",
    "        if np.linalg.norm(v) > eps:\n",
    "            # Normalize vector v and append it to U\n",
    "            U = np.hstack((U, v / np.linalg.norm(v)))\n",
    "    \n",
    "    return U\n",
    "\n",
    "h = hilbert_matrix(7)\n",
    "\n",
    "h_basis_mod = modified_gram_schmidt(h)\n",
    "\n",
    "print(f\"Basis for hilbert matrix WITH MODIFIED GS is: {h_basis_mod}\")\n",
    "\n",
    "print(f\"If the following matrix is nearly the identity matrix, our basis is orthogonal: \")\n",
    "error = h_basis_mod@h_basis_mod.T\n",
    "print(error)\n",
    "\n",
    "L_1_norm = la.norm(error - np.identity(7))\n",
    "print(f\"The L_1 norm of the error matrix of basis WITH MODIFIED GS is: {L_1_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.5       , 0.33333333, 0.25      , 0.2       ,\n",
       "        0.16666667, 0.14285714],\n",
       "       [0.5       , 0.33333333, 0.25      , 0.2       , 0.16666667,\n",
       "        0.14285714, 0.125     ],\n",
       "       [0.33333333, 0.25      , 0.2       , 0.16666667, 0.14285714,\n",
       "        0.125     , 0.11111111],\n",
       "       [0.25      , 0.2       , 0.16666667, 0.14285714, 0.125     ,\n",
       "        0.11111111, 0.1       ],\n",
       "       [0.2       , 0.16666667, 0.14285714, 0.125     , 0.11111111,\n",
       "        0.1       , 0.09090909],\n",
       "       [0.16666667, 0.14285714, 0.125     , 0.11111111, 0.1       ,\n",
       "        0.09090909, 0.08333333],\n",
       "       [0.14285714, 0.125     , 0.11111111, 0.1       , 0.09090909,\n",
       "        0.08333333, 0.07692308]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error rate for Truncaded SVD: 0.11160714285714286\n",
      "Final error rate for Regularized LS: 0.04799107142857143\n",
      "Final error rate for AUGMENTED X Truncaded SVD: 0.10267857142857142\n",
      "Final error rate for AUGMENTED X Regularized LS: 0.046875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "d = np.load(\"face_emotion_data.npz\")\n",
    "X = d['X']\n",
    "y = d['y']\n",
    "\n",
    "n, p = np.shape(X)\n",
    "\n",
    "# error rate for regularized least squares\n",
    "error_RLS = np.zeros((8, 7))\n",
    "# error rate for truncated SVD\n",
    "error_SVD = np.zeros((8, 7))\n",
    "\n",
    "# SVD parameters to test\n",
    "k_vals = np.arange(9) + 1\n",
    "param_err_SVD = np.zeros(len(k_vals))\n",
    "\n",
    "def get_pseudo_inverse_weight(k, X, y):\n",
    "    U, S, Vt = la.svd(X,full_matrices=False)\n",
    "    S_p = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        S_p[i][i] = 1 /S[i]\n",
    "    psuedo_inverse = Vt.T[:, :k]@(S_p@U[:, :k].T)\n",
    "    return np.dot(psuedo_inverse, y)\n",
    "\n",
    "def get_prediction(X, w):\n",
    "    y_raw = np.dot(X, w)\n",
    "    y = np.where(y_raw >=0, 1, -1)\n",
    "    return y\n",
    "\n",
    "subset_size = n // 8\n",
    "\n",
    "def get_error_SVD(X, y):\n",
    "    for i in range(8):\n",
    "        test_indices = np.arange(i * subset_size, (i+1) * subset_size) % n\n",
    "        remaining_indices = np.setdiff1d(np.arange(n), test_indices)\n",
    "        \n",
    "        for j in range(7):\n",
    "            val_indices = remaining_indices[j * subset_size:(j+1) * subset_size] % n\n",
    "            train_indices = np.setdiff1d(remaining_indices, val_indices)\n",
    "            \n",
    "            X_train, y_train = X[train_indices], y[train_indices]\n",
    "            X_val, y_val = X[val_indices], y[val_indices]\n",
    "            X_test, y_test = X[test_indices], y[test_indices]\n",
    "            \n",
    "            best_k = None\n",
    "            best_error = np.inf\n",
    "            for k in k_vals:\n",
    "                w_k = get_pseudo_inverse_weight(k, X_train, y_train)\n",
    "                y_pred = get_prediction(X_val, w_k)\n",
    "                error = np.mean(y_pred != y_val)\n",
    "                if error < best_error:\n",
    "                    best_k = k\n",
    "                    best_error = error\n",
    "            \n",
    "            w_best_k = get_pseudo_inverse_weight(best_k, X_train, y_train)\n",
    "            y_pred_test = get_prediction(X_test, w_best_k)\n",
    "            test_error = np.mean(y_pred_test != y_test)\n",
    "            \n",
    "            error_SVD[i, j] = test_error\n",
    "\n",
    "get_error_SVD(X, y)\n",
    "mean_err_svd = np.mean(error_SVD.flatten())\n",
    "print(f\"Final error rate for Truncaded SVD: {mean_err_svd}\")\n",
    "\n",
    "# RLS parameters to test\n",
    "lambda_vals = np.array([0, 0.5, 1, 2, 4, 8, 16])\n",
    "param_err_RLS = np.zeros(len(lambda_vals))\n",
    "\n",
    "def get_reg_ls_weight(X, y, lambda_val, k=9):\n",
    "    U, S, Vt = la.svd(X,full_matrices=False)\n",
    "    S_p = np.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        S_p[i][i] = S[i] /(S[i]**2 + lambda_val)\n",
    "    w_lambda = Vt.T[:, :k]@(S_p@U[:, :k].T)\n",
    "    return np.dot(w_lambda, y)\n",
    "\n",
    "def get_error_RLS(X, y):\n",
    "    for i in range(8):\n",
    "        test_indices = np.arange(i * subset_size, (i+1) * subset_size) % n\n",
    "        remaining_indices = np.setdiff1d(np.arange(n), test_indices)\n",
    "        \n",
    "        for j in range(7):\n",
    "            val_indices = remaining_indices[j * subset_size:(j+1) * subset_size] % n\n",
    "            train_indices = np.setdiff1d(remaining_indices, val_indices)\n",
    "            \n",
    "            X_train, y_train = X[train_indices], y[train_indices]\n",
    "            X_val, y_val = X[val_indices], y[val_indices]\n",
    "            X_test, y_test = X[test_indices], y[test_indices]\n",
    "            \n",
    "            best_k = None\n",
    "            best_error = np.inf\n",
    "            for lambda_val in lambda_vals:\n",
    "                w_k = get_reg_ls_weight(X_train, y_train, lambda_val, X_train.shape[1])\n",
    "                y_pred = get_prediction(X_val, w_k)\n",
    "                error = np.mean(y_pred != y_val)\n",
    "                if error < best_error:\n",
    "                    best_k = lambda_val\n",
    "                    best_error = error\n",
    "            \n",
    "            w_best_k = get_reg_ls_weight(X_train, y_train, best_k)\n",
    "            y_pred_test = get_prediction(X_test, w_best_k)\n",
    "            test_error = np.mean(y_pred_test != y_test)\n",
    "            \n",
    "            error_RLS[i, j] = test_error\n",
    "\n",
    "get_error_RLS(X, y)\n",
    "mean_err_rls = np.mean(error_RLS.flatten())\n",
    "print(f\"Final error rate for Regularized LS: {mean_err_rls}\")\n",
    "\n",
    "'''\n",
    "Now for augmented X:\n",
    "'''\n",
    "\n",
    "X_rand = X @ np.random.rand(9, 3)\n",
    "X = np.hstack((X, X_rand))\n",
    "    \n",
    "get_error_SVD(X, y)\n",
    "mean_err_svd = np.mean(error_SVD.flatten())\n",
    "print(f\"Final error rate for AUGMENTED X Truncaded SVD: {mean_err_svd}\")\n",
    "\n",
    "get_error_RLS(X, y)\n",
    "mean_err_rls = np.mean(error_RLS.flatten())\n",
    "print(f\"Final error rate for AUGMENTED X Regularized LS: {mean_err_rls}\")\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
